{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Say hello to machine learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic machine learning tutorial, using scikit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @ PyConPL 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[1 Preparation of the development environment](#1-Preparation-of-the-development-environment)<br/>\n",
    "[2 Introduction to Machine Learning](#2-Introduction-to-Machine-Learning)<br/>\n",
    "[3 Introduction to classification](#3-Introduction-to-classification)<br/>\n",
    "[4 Introduction to regression](#4-Introduction-to-regression)<br/>\n",
    "[5 Stand alone project](#5-Project-time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preparation of the development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As everyone can have several versions of Python installed in the system, we use somewhat convoluted way of installing packages to add them to the same version of Python interpreter being used to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas scipy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few libraries that will be used during the workshop. <br/>\n",
    "**Please confirm that you can import all of them successfully.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = urllib.request.urlretrieve('https://s3.eu-central-1.amazonaws.com/ml-workshop-pycon-2018/startups.csv', 'startups.csv')\n",
    "_ = urllib.request.urlretrieve('https://s3.eu-central-1.amazonaws.com/ml-workshop-pycon-2018/orthopedic_patients_3C.csv', 'orthopedic_patients_3C.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some \"global\" constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "STARTUPS_DATASET_PATH = './startups.csv'\n",
    "ORTHOPEDIC_PATIENTS_DATASET_PATH = './orthopedic_patients_3C.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility (visualisation) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"\n",
    "    Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def _plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"\n",
    "    Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_decision_regions(clf, X, y, title='', x_label='', y_label='', figsize=(6, 6)):\n",
    "    \"\"\"\n",
    "    Plots decision regions for passed as argument classifier and data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: a (fitted) classifier\n",
    "    X: matrix of samples, that will be used as classifier input (note that only 2 first features will be used)\n",
    "    title: title for a generated plot\n",
    "    x_label: label for x axis of generated plot\n",
    "    y_label: label for y axis of generated plot\n",
    "    figsize: size of generated plot in inches, 2 element tuple (width, height)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = _make_meshgrid(X0, X1)\n",
    "    \n",
    "    _plot_contours(plt.gca(), clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "    \n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Introduction to classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> all concepts, that are present/described here, will be more fully described during workshop and on slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "print(iris_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris_dataset.data\n",
    "y = iris_dataset.target\n",
    "\n",
    "X = X[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default, completely random split with 25% of data assigned to test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "print('Class labels:', np.unique(y_train))\n",
    "print('Class counts:', np.unique(y_train, return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... vs stratified (preserving class distribution) split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED,\n",
    "                                                   stratify=y)\n",
    "\n",
    "\n",
    "print('Class labels:', np.unique(y_train))\n",
    "print('Class counts:', np.unique(y_train, return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "y_train_pred = knn_clf.predict(X_train)\n",
    "y_test_pred = knn_clf.predict(X_test)\n",
    "print('Train acc:', accuracy(y_train, y_train_pred))\n",
    "print('Test acc:', accuracy(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing this code by ourselves, we can use scikit help in this case too - it contains implementations of most popular metrics (some of which are much more complex and harder to code than the accuracy used above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "y_test_pred = knn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are dealing with two features only, we can also check, how the algorithm decision regions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(knn_clf, X_test, y_test, 'KNN decision regions', 'sepal length [cm]', 'sepal_width [cm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbors classifier is only one of many ML algorithms. Let's use another one, logistic regression, which is a linear classifier to see how well it will tackle the problem, and what the decision boundaries will be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "log_reg_clf = LogisticRegression()\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "y_train_pred = log_reg_clf.predict(X_train)\n",
    "y_test_pred = log_reg_clf.predict(X_test)\n",
    "print('Train acc:', accuracy_score(y_train, y_train_pred))\n",
    "print('Test acc:', accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(log_reg_clf, X_test, y_test, 'KNN decision regions', 'sepal length [cm]', 'sepal_width [cm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of data we use for model training influence its performance greatly. Besides them, algorithms themselves have also another set of parameters, called hyperparameters, that affect how they work/optimize given problem. We will test it in one of the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=1)  # default number of neighbors is 5\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = knn_clf.predict(X_train)\n",
    "y_test_pred = knn_clf.predict(X_test)\n",
    "print('Train acc:', accuracy(y_train, y_train_pred))\n",
    "print('Test acc:', accuracy(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "- Explore in more exhaustive way, how the change of *k* parameter in KNN classifier affects achieved accuracy. Fit the classifier with all possible *k* values in range [1, 3, 5 ... 51] and check the accuracy. <br/> **Bonus**: Plot the results you obtained.\n",
    "- Fit the data using yet another classifier - Decision Tree (located in *sklearn.tree*). Do you need to change much of a code to test another algorithm?\n",
    "- Remember how we selected only two features of Iris dataset after loading? It was done for visualisation purposes, but more features can definitely help us improve our score. Investigate how different models will perform using data with all features. <br/>**Bonus**: After that, you can also play with the hyperparameters - try to obtain as high accuracy on test set as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Introduction to regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset which we've used earlier is available directly in the library and \"ready to go\". The reality is often not so pleasant, and data comes with many problems, like missing or inconsistent values, that must be handled first. In fact, it often takes even 80-90% of the time when working on a project. Fortunately, there is plenty of libraries that simplify working with data, i.e. pandas which we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(STARTUPS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV is one of the simplest formats to handle. Sometimes datasets contain data stored in various binary formats that are not as straightforward to load.\n",
    "\n",
    "Remember to always look for a ready-to-use solution before you start writing your own data parser. It will save you a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Startups Database\n",
    "=================\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Dataset Characteristics:\n",
    "    :Number of Instances: 50\n",
    "    :Number of Attributes: 5 (4 numeric and 1 textual)\n",
    "    :Attribute Information:\n",
    "        - Research & Developement Spend in USD\n",
    "        - Administration Spend in USD\n",
    "        - Merketing Spend in USD\n",
    "        - State in which the startup is located (3 possible values)\n",
    "\t- Profit in USD\n",
    "\n",
    "This datasets describes some attributes of 50 startups. The task is to perform\n",
    "a regression that will predict the overall profit of the startup using all\n",
    "other information.\n",
    "\n",
    "Please note that since this dataset is very small, it can only be used with\n",
    "simple regression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation - data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we loaded the data, let's take a peek at how does it look. We can also generate some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see some inconsistencies. R&D and Administration costs have less non-null values than other properties.\n",
    "\n",
    "In order to perform the training we will get rid of the rows that lack some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(data).any(axis=1)\n",
    "# data[pd.isnull(data).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the length (number of rows) of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's drop the rows with missing information and check the length again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation - encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: we go rid of samples with missing data, but we need our features to be numeric matrix - condition which might not be exactly fulfilled looking at 'California' in state column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['R&D Spend', 'Administration', 'Marketing Spend', 'State']].values\n",
    "y = data['Profit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "state_le = LabelEncoder()\n",
    "state_le.fit(X[:, 3])\n",
    "X[:, 3] = state_le.transform(X[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_le.classes_\n",
    "state_le.inverse_transform([0, 1, 2])  # we can inverse performed transformation at any time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our data but there is a problem: should Florida be numerically greater than New York? What does it even mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "state_ohe = OneHotEncoder()\n",
    "state_ohe.fit(X[:, 3].reshape(-1,1))  # OneHotEncoder require matrix input - hence reshape\n",
    "encoded = state_ohe.transform(X[:, 3].reshape(-1,1)).toarray()\n",
    "encoded = np.delete(encoded, 0, axis=1)  # we drop one column as it is redundant and can be inferred from remaining ones\n",
    "X = np.delete(X, 3, axis=1) \n",
    "X = np.hstack([X, encoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation - feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (one/two sentences about feature scaling itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)  # Note that we only fit scaler on train data - same as with model fitting.\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done, but did something actually happen? Do feature scalers do something? (Do they know something?) Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now our data is finally ready to be passed to the model, weee :) <br/> By the way, you will probably agree that even for a simple data quite some work was needed to be done first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Quick exercise - test all of metrics imported above. Look up what do they do in documantation if names aren't intuitive\n",
    "# enough. Which one of them do you think is the best and why?\n",
    "\n",
    "\n",
    "y_train_pred = lin_reg.predict(X_train)\n",
    "y_test_pred = lin_reg.predict(X_test)\n",
    "print('Train metric:', some_metric(y_train, y_train_pred))\n",
    "print('Test metric:', some_metric(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can't plot the results in 2D - but we can check weights assigned to features and intercept, which will give us  idea what is important to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lin_reg.coef_)\n",
    "print(lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What other ways of dealing with missing data besides sample deletion you can come up with? What are their potential pros and cons? (Tip: some methods were already mentioned during walking through slides). \n",
    "- Analyse parameters of the trained linear regression model. As some of them are less significant then the others, try to  simplify model to use less features. (If possible, try to visualise fitted line/points after retraining)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Project time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now is the time you are in post-credits scene saying: \"Fine, I'll do it myself\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - task description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of a tips for a start: identify kind of a task, load and analyse data. Don't be scared to consult previous parts of the notebook. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
